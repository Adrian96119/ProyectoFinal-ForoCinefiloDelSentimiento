{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"../datasets/reseñas_limpio.csv\") #importo mi csv limpio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARA ENTRENAR EL MODELO, PRIMERO NECESITAMOS ETIQUETAR CON 1 Y 0 LA COLUMNA DE SENTIMENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = movies['sentiment'] #NUESTRA VARIABLE Y CON 0 Y 1 PARA LAS ETIQUETAS DEL SENTIMIENTO\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y))) #CON ESTO LO QUE HAGO ES ESTABLECER EN 1 LOS POSITIVOS \n",
    "#Y EN 0 LOS NEGATIVOS.\n",
    "\n",
    "X = movies[\"review\"] #LA VARIABLE X SERAN LAS LISTAS DE PALABRAS DE RESEÑAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "#ESTABLEZCO UNA PROPORCION 0.20 PARA EL CONJUNTO DE PRUEBA Y 0.80 PARA EL DE ENTRENAMIENTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (10000,) (40000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)#compruebo que todas las dimensiones están bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=5000)  #establezco el número máximo de palabras que se pueden conservar, según la frecuencia de palabras.\n",
    "token.fit_on_texts(X_train) #actualizo el vocabulario en el x train\n",
    "\n",
    "X_train = token.texts_to_sequences(X_train) #transformamos nuestros tokens en numeros para x train y para x test\n",
    "X_test = token.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debemos establecer ahora un tamaño igual para todas las listas, ya que hay reseñas mas largas o menos largas asique vamos\n",
    "#a establecer una longitud de 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91098"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(token.word_index) + 1 #Todas las palabras y sus índices se almacenarán en un diccionario al que podre acceder \n",
    "\n",
    "\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=100) #dejo un maximo de 100 palabras por lista.\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=100)\n",
    "vocab_size #nuestro corpus tiene 91098 palabras únicas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape #ahora tenemos 40000 listas con 100 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape #igual que en el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo simple multicapa de Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               800250    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 960,501\n",
      "Trainable params: 960,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 32, input_length=100))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3561 - accuracy: 0.8382 - val_loss: 0.3007 - val_accuracy: 0.8735\n",
      "Precisión: 87.35%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=32,\n",
    "verbose=1)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Precisión: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9405999779701233"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train, verbose=0)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8734999895095825"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test,verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVAMOS QUE EL MODELO GENERALIZA BASTANTE BIEN YA QUE HAY POCA DIFERENCIA ENTRE COMO APRENDIÓ CON EL ENTRENAMIENTO Y \n",
    "#CON LA PARTE DE PRUEBA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHORA DEBEMOS REVERTIR LOS NUMEROS A LA LISTA DE TEXTOS ORIGINAL PARA SABER SI ACERTÓ CON LAS PREDICCIONES, YA QUE SI NO \n",
    "HACEMOS ESTA CONVERSIÓN NO PODEMOS SABER POR LOS NÚMEROS QUE CONTEXTO TIENE ESA RESEÑA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = token.sequences_to_texts(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = token.sequences_to_texts(X_test) #paso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01607266], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.predict(X_test) #HAGO PREDICCIONES EN EL CONJUNTO DE PRUEBA \n",
    "pred_test[9] #COJEMOS EL NUMERO 9 Y VEMOS QUE POR LA PUNTUACIÓN, AL ESTAR MÁS CERCA DEL 0, CASI PEGADO, NOS ACLARA QUE ES\n",
    "#UNA RESEÑA MUY NEGATIVA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'offered' 'hope' 'decided' 'give' 'minutes' 'introduction' 'film' 'director' 'added' 'bit' 'solid' 'acting' 'performance' 'raised' 'presence' 'beyond' 'welcome' 'eye' 'candy' 'ultimately' 'obvious' 'low' 'budget' 'film' 'poorly' 'shot' 'scenes' 'pace' 'slapstick' 'certain' 'moments' 'favourite' 'movies' 'time' 'low' 'budget' 'one' 'also' 'deals' 'guys' 'dream' 'luck' 'however' 'money' 'actors' 'save' 'terrible' 'movie' 'could' 'cult' 'movie' 'laughed' 'loud' 'scenes' 'involving' 'joe' 'character' 'particular' 'scenes' 'terribly' 'clich' 'still' 'funny' 'rich' 'characters' 'house' 'story' 'towards' 'final' 'moments' 'see' 'great' 'stage' 'play' 'film' 'makers' 'best' 'translate' 'celluloid' 'simply' 'work' 'laughed' 'loud' 'scenes' 'one' 'liners' 'think' 'first' 'minutes' 'expectations' 'degree' 'would' 'laughed' 'anything' 'unless' 'stuck' 'coffee' 'pick' 'see' 'bargain'\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews[9]  #podemos observar que por el contexto de las palabras, la reseña es negativa, y además bastante, como\n",
    "#predijo bien nuestro modelo puntuándola bajisima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probemos con alguna positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76632994], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test[25] #esta la puntúan positiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'went' 'see' 'hamlet' 'jobs' 'figured' 'hours' 'would' 'great' 'fan' 'branagh' 'dead' 'henry' 'completely' 'direction' 'acting' 'cinematography' 'film' 'captured' 'like' 'reviews' 'hours' 'passes' 'branagh' 'play' 'hamlet' 'hamlet' 'born' 'watch' 'film' 'constantly' 'trying' 'find' 'looked' 'noticed' 'able' 'move' 'camera' 'hall' 'mystery' 'movie' 'shot' 'shame' 'columbia' 'released' 'widescreen' 'version' 'vhs' 'dvd' 'player' 'take' 'titanic' 'day' 'columbia' 'listening' 'put' 'film' 'way' 'watched' 'know' 'happened' 'oscars' 'best' 'picture' 'best' 'actor' 'best' 'direction' 'best' 'cinematography' 'films' 'watching' 'felt' 'sorry' 'branagh' 'oscars' 'tribute' 'shakespeare' 'screen' 'giving' 'tribute' 'branagh' 'bringing' 'one' 'greatest' 'films' 'time'\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews[25] #tampoco se equivocó ya que la ponen bien por el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAMOS A GUARDAR NUESTRO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en el PC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "#serializan los pesos (weights) para HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Modelo guardado en el PC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
