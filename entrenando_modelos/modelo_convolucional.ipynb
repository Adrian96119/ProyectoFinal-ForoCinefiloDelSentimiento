{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"../datasets/reseñas_limpio.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = movies['sentiment'] #NUESTRA VARIABLE Y CON 0 Y 1 PARA LAS ETIQUETAS DEL SENTIMIENTO\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y))) #CON ESTO LO QUE HAGO ES ESTABLECER EN 1 LOS POSITIVOS \n",
    "#Y EN 0 LOS NEGATIVOS.\n",
    "\n",
    "X = movies[\"review\"] #LA VARABLE X SERAN LAS LISTAS DE PALABRAS DE RESEÑAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "#ESTABLEZCO UNA PROPORCION 0.20 PARA EL CONJUNTO DE PRUEBA Y 0.80 PARA EL DE ENTRENAMIENTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (10000,) (40000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)#compruebo que todas las dimensiones están bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=5000)  #establezco el número máximo de palabras que se pueden conservar, según la frecuencia de palabras.\n",
    "token.fit_on_texts(X_train) #actualizo el vocabulario en el x train\n",
    "\n",
    "X_train = token.texts_to_sequences(X_train) #transformamos nuestros tokens en numeros para x train y para x test\n",
    "X_test = token.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91098"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(token.word_index) + 1 #Todas las palabras y sus índices se almacenarán en un diccionario al que podre acceder \n",
    "\n",
    "\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=100) #dejo un maximo de 100 palabras por lista.\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=100)\n",
    "vocab_size #nuestro corpus tiene 91098 palabras únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape #ahora tenemos 40000 listas con 100 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape #igual que en el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 96, 128)           20608     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250)               1536250   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 1,717,109\n",
      "Trainable params: 1,557,109\n",
      "Non-trainable params: 160,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(5000,32, input_length=100 , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(128,5,activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 0.3043 - accuracy: 0.8701 - val_loss: 0.5560 - val_accuracy: 0.7483\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 0.2326 - accuracy: 0.9051 - val_loss: 0.6331 - val_accuracy: 0.7446\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6331 - accuracy: 0.7446\n",
      "Precisión: 74.46%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train, validation_data=(X_test,y_test),epochs = 2, batch_size=32,verbose=1)\n",
    "scores = model.evaluate(X_test,y_test,verbose=1)\n",
    "print(\"Precisión: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
